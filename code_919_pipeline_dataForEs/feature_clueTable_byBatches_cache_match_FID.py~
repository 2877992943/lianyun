#! -*- coding:utf-8 -*-

import MySQLdb,os
import random,cPickle
import csv
import pandas as pd

from MySQLdb import cursors
import numpy as np
from clue import *

from base import CategoricalTextMixIn, ClueFeature, FieldExistsMixIn, ParseNumberMixIn,get_clues
import pylab as plt
import sys,time
reload(sys)
sys.setdefaultencoding('utf8')

db = MySQLdb.connect(host='rds5943721vp4so4j16ro.mysql.rds.aliyuncs.com',
                     user='yunker',
                     passwd="yunker2016EP",
                     db="xddb",
                     use_unicode=True,
                     charset='utf8',
                     cursorclass=cursors.DictCursor)

def get_all_idx():
    '''
    Get all the Clue_Id
    '''
    sql = "SELECT clue_id from crm_t_clue limit 10000"
    df=pd.read_sql(sql,db)
    df.to_csv("../allClueId.csv",index=False,encoding='utf-8')

def get_negative_idx(clueId): #too slow
    print '1', clueId[0],len(clueId)
    '''
    Get negative   Clue_Id
    '''
    sql = """SELECT clue_id from crm_t_clue
            where clue_id not in ('%s')
            limit 10"""

    df=pd.read_sql(sql % "','".join(clueId),db)
    #df=pd.read_sql(sql,db)
    df.to_csv("../negativeClueId.csv",index=False,encoding='utf-8')



def get_positive_id():
    '''
    Get the positive_ids from phone_response_clue.txt ->phone ,id,
    '''

    positive_responses = set(['QQ', '短信', '注册', '邮箱', '预约' ,'微信'])
    with open('../tianxiang/phone_response_clue.txt') as f:
        lines = f.read().split('\n')[:-1]
    oids = [l.split(',')[2] for l in lines if l.split(',')[1] in positive_responses]#id
    teles = [l.split(',')[0] for l in lines if l.split(',')[1] in positive_responses]#tele

    pd.DataFrame({'clueId':oids,'teles':teles}).to_csv("../clueId_tele.csv",index=False,encoding='utf-8')
    return oids,teles



def get_clue_idx(tels):
    '''
    Get Clue_Ids by Clue_Entry_Cellphone
    '''

    sql = """
        SELECT CLue_Id
        FROM crm_t_clue
        WHERE Clue_Entry_Cellphone IN
              ('%s')
    """
    df=pd.read_sql(sql % "','".join(tels),db)
    df.to_csv("../teleQueryClueId.csv",index=False,encoding='utf-8')




def save2pickle(c,name):
    write_file=open(str(name),'wb')
    cPickle.dump(c,write_file,-1)#[ (timestamp,[motion,x,y,z]),...]
    write_file.close()

def load_pickle(path_i):
    f=open(path_i,'rb')
    data=cPickle.load(f)#[ [time,[xyz],y] ,[],[]...]
    f.close()
    #print data.__len__(),data[0]
    return data




def try_lr(X,y):
    import math

    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import cross_val_score
    from sklearn.metrics import roc_auc_score
    from sklearn.ensemble import GradientBoostingClassifier

    #######
    ss = StandardScaler(with_mean=False)
    X_norm = ss.fit_transform(X)
    lr0 = LogisticRegression(penalty='l1', class_weight='balanced', random_state=0)
    ########
    scorelist=cross_val_score(lr0, X_norm, y,lambda m, x, y: roc_auc_score(y, m.predict_proba(x)[:,1].T),cv=3)
    print scorelist
    lr0.fit(X_norm,y)

    ##########
    # select feature
    selected_fids = [];
    for i, e in enumerate(lr0.coef_[0]): #coefficient [n_class,n_feature] w parameter matrix
        if abs(e) != 0:
            selected_fids.append(i)

    print 'lr select non zero fea',len(selected_fids)
    ##############
    # select non 0 w parameter
    wMatrix=lr0.coef_ ;print 'w',wMatrix.shape
    fid,fv=remove_zeroFea_sort(np.squeeze(wMatrix))#[1,d]->[d,]
    #feaInd=np.unique(np.nonzero(wMatrix)[1]) ;print 'non 0 w lr',feaInd.shape #[2,5,9,11,..]
    #feaVal=np.squeeze(wMatrix[:,feaInd]); #[d,]
    ### sort by abs
    #feaVal_abs=np.abs(feaVal)
    #indice=np.argsort(feaVal)[::-1]
    #FeaIndSortedByVal=feaInd[indice]
    return fid,fv



    #######
    #X_norm_selected = X_norm[:, selected_fids]
    #print X_norm_selected.shape
    #save2pickle([X_norm_selected,y],'../selecteXy')
    #return selected_fids#list

def remove_zeroFea_sort(importances): #[d,]
    nonzeroFidx=np.nonzero(importances)[0];print 'non 0 fea',nonzeroFidx.shape
    #### removed zero -- nonzeroFidx:importances[nonzeroFidx]
    indices = np.argsort(importances[nonzeroFidx])[::-1]
    nonzeroFidx_sort=nonzeroFidx[indices]
    """
    for i in range(indices.shape[0])[:10]:
        print 'feature ind',nonzeroFidx[indices[i]],importances[nonzeroFidx][indices[i]],nonzeroFidx_sort[i]
    """
    return nonzeroFidx[indices],importances[nonzeroFidx][indices]
def rf_important_fea(xtrain,ytrain):
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import cross_val_score
    from sklearn.metrics import roc_auc_score
    ss = StandardScaler(with_mean=False)
    X_norm = xtrain#ss.fit_transform(xtrain)
    clf = RandomForestClassifier(n_estimators=10,max_depth=10)
    ########
    scorelist=cross_val_score(clf, X_norm, ytrain,lambda m, x, y: roc_auc_score(ytrain, m.predict_proba(X_norm)[:,1].T),cv=3)
    print scorelist
    clf.fit(X_norm,ytrain)
    ###########
    #rf important feature
    ###############

    from sklearn.ensemble import RandomForestClassifier

    clf = clf.fit(X_norm, ytrain)


    importances = clf.feature_importances_;#print 'important',importances.shape#[dim,] same shape as dim

    #nonzeroFidx=np.nonzero(importances)[0];print 'non 0 fea rf',nonzeroFidx.shape
    #### remove zero -- nonzeroFidx:importances[nonzeroFidx]
    #indices = np.argsort(importances[nonzeroFidx])[::-1]
    #nonzeroFidx_sort=nonzeroFidx[indices]
    #for i in range(indices.shape[0])[:10]:
    #    print 'feature ind',nonzeroFidx[indices[i]],importances[nonzeroFidx][indices[i]],nonzeroFidx_sort[i]
    fid,fv=remove_zeroFea_sort(importances)

    #############
    # filter feature
    X_norm=X_norm[:,fid];print X_norm.shape
    clf1=RandomForestClassifier(n_estimators=10,max_depth=10)
    scorelist=cross_val_score(clf1, X_norm, ytrain,lambda m, x, y: roc_auc_score(ytrain, m.predict_proba(X_norm)[:,1].T),cv=3)
    print scorelist
    return fid,fv



def draw_bar_importantF(FeaIndSortedByVal,FeaValSortedByVal,title_clf):
    plt.figure()
    plt.title("Feature importances %s"%title_clf)
    plt.bar(range(len(FeaIndSortedByVal)), FeaValSortedByVal,color="r")
    plt.xticks(range(len(FeaIndSortedByVal)),FeaIndSortedByVal,rotation='vertical',size='xx-small')#[0,1,2]->[a,c,r] 'vertical'

    #plt.xlim([-1, xtrain.shape[1]])
    #plt.xlim([-1, 15])

if __name__ == "__main__":
    time_start=time.time()

    """

    #############
    #cache by batch raw data,no feastr
    #### given clueId->{clueid:{feaStr:v,,,,}
    df=pd.read_csv("../backup/allClue.oid",encoding='utf-8');

    all_clueId=np.unique(np.squeeze(df.values));print 'all clueid',all_clueId.shape

    ########## query calc fea  ->{clueid:{feaStr:feavalue...}
    from base import ClueFeature
    from clue import *

    ## ### batches of cache
    numall=all_clueId.shape[0]
    num_batches=numall/100000+1
    batch=0


    for i in range(num_batches)[:]: # 0 1 2 3
        batch=i
        features = ClueFeature.__subclasses__()
        print 'batch',i
        model_clueid=all_clueId.tolist()[i*100000:(i+1)*100000]
        #model_clueid=model_clueid[:2]
        get_clues(model_clueid[:]) # into cache
        from generate_clue_str import gen_svmlight
        #gen_svmlight('strDict_clue_'+str(i), features, model_clueid[:],'clue')

    ### last batch
    features = ClueFeature.__subclasses__()

    model_clueid=all_clueId.tolist()[2600000:];print model_clueid.__len__()
    get_clues(model_clueid[:]) # into cache
    """









    time_end=time.time()
    print 'time... %f sec'%(time_end-time_start)



    ############ load dict
    #idFeaDict=pd.read_pickle('../clueTable_dict/id_feaDict_strDict_clue_0')
    #print idFeaDict



    ############3
    # each cache preprocess
    fpath='../cache_batches/'
    outpath='../raw_batches/'
    """
    ## make same len of cid_fid_dict and allclueid
    df=pd.read_csv("../backup/allClue.oid",encoding='utf-8');
    all_clueId_list=np.unique(np.squeeze(df.values)).tolist();print 'all clueid',len(all_clueId_list)
    rowdictList=[{}]*len(all_clueId_list)
    cid_emptyRow_dict=dict(zip(all_clueId_list,rowdictList))
    ## cid_fid dict
    fid_dict,name2fid_lowDim=pd.read_pickle('../all_cid_fid_dict');print 'fid', len(fid_dict)
    cid_emptyRow_dict.update(fid_dict)
    pd.to_pickle(cid_emptyRow_dict,'../all_cid_fid_dict_includeEmpty')

    """


    fid_dict=pd.read_pickle('../all_cid_fid_dict_includeEmpty')
    """

    batch=0
    for filename in os.listdir(fpath)[:]:
        batch+=1;print batch

        ## each batch
        cachei=pd.read_pickle(fpath+filename);print 'cachei',len(cachei)
        batch_cid_raw_fid_dic=cachei.copy()
        num=0
        for cid,rowdict in cachei.items()[:]:
            #num+=1
            #print rowdict

            #if 'Creat_Time' in rowdict.keys():
            #    del rowdict['Creat_Time']
            rowdict['feature_value']={}
            #if cid in fid_dict.keys():
            rowdict['feature_value']=fid_dict[cid]

            #####
            batch_cid_raw_fid_dic[cid]=rowdict
            #if num%10000==0:print num
        ####
        pd.to_pickle(batch_cid_raw_fid_dic,outpath+'raw_fid_%s'%str(batch))
    """



    endt=time.time()
    print 'time',endt-time_start






























